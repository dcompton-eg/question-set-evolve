{
  "title": "Senior Go Backend Engineer - Recruiter Phone Screen",
  "description": "A 10-minute phone screen for senior Go engineers designed for non-technical recruiters. Questions assess hands-on Go project experience, Go-specific concepts (concurrency and error handling), and production deployment/operational considerations. Questions are scripted for easy reading aloud and designed to elicit specific examples from real projects.",
  "target_role": "Senior Go (Golang) Backend Engineer",
  "total_time_minutes": 10,
  "questions": [
    {
      "question_id": "Q1_PROJECT_EXPERIENCE",
      "question_text": "Tell me about a significant Go backend project you've built or maintained in your career. What was the biggest technical challenge you had to solve in that project, and how did you approach it?",
      "category": "hands-on experience",
      "difficulty": "medium",
      "time_allocation_minutes": 3,
      "follow_up_questions": [],
      "what_to_look_for": "STRONG ANSWER: Candidate describes a specific project with concrete context (team size, scale, timeline, business impact). They identify a real technical challenge (not trivial) and explain their approach with specific technical details. They may mention trade-offs considered, constraints, or why their solution was appropriate. Examples: scaling challenges, architectural decisions, integration complexity, performance optimization.\n\nWEAK ANSWER: Vague project descriptions (\"worked on a backend service\"). Generic challenges that could apply to any language. No specifics about what was built or why it mattered. Answers like \"we just used Go because it was fast\" without context. Cannot articulate what the project actually did.\n\nMINIMUM VIABLE EVIDENCE: Candidate must name a specific project type, describe what it did, and explain at least one concrete technical challenge they personally solved (not just what the team did).\n\nTRANSCRIPT SIGNALS TO LISTEN FOR:\n- Specific numbers: \"served X requests/second\", \"team of X engineers\", \"reduced latency by X%\"\n- Specific Go features mentioned in context: \"used goroutines for\", \"channels to coordinate\", \"context for cancellation\"\n- Problem-solving language: \"we realized\", \"the challenge was\", \"we tried X but\", \"we ended up\"\n- Business/operational context: \"customer impact\", \"SLA\", \"scale\", \"throughput\"\n- Personal ownership: \"I designed\", \"I implemented\", \"I debugged\"\n- Realistic constraints: \"memory limits\", \"latency budget\", \"deployment window\", \"team skill level\""
    },
    {
      "question_id": "Q2_GO_CONCEPTS",
      "question_text": "Go has unique features for handling concurrency and errors. Tell me about a time when you used goroutines, channels, or Go's error handling approach to solve a real problem in production. What would have been harder or different if you were using a different programming language?",
      "category": "Go-specific concepts",
      "difficulty": "medium",
      "time_allocation_minutes": 3,
      "follow_up_questions": [],
      "what_to_look_for": "STRONG ANSWER: Candidate describes a specific production scenario where they used goroutines/channels or error handling patterns. They explain WHY that Go feature was the right choice—not just that they used it. They can articulate what would be different in another language (Java, Python, Node.js, etc.) and why Go's approach was better for that specific problem. Examples: handling concurrent requests, coordinating multiple operations, graceful shutdown, error propagation patterns.\n\nWEAK ANSWER: Textbook explanation of goroutines or error handling without connecting to a real problem. \"Goroutines are lightweight threads\" without context. Cannot explain why Go's approach matters. Vague answers like \"we used concurrency because Go is concurrent.\" No comparison to other languages or understanding of trade-offs.\n\nMINIMUM VIABLE EVIDENCE: Candidate must describe a specific production use case where they personally used goroutines/channels OR Go's error handling pattern, and explain why that feature was appropriate for that problem (not just what it does).\n\nTRANSCRIPT SIGNALS TO LISTEN FOR:\n- Specific concurrency patterns: \"worker pool\", \"fan-out/fan-in\", \"graceful shutdown\", \"context cancellation\", \"select statement\"\n- Error handling patterns: \"error wrapping\", \"error checking at each step\", \"error types\", \"panic vs error\"\n- Production context: \"high traffic\", \"concurrent requests\", \"race condition\", \"deadlock\", \"timeout\"\n- Comparative language: \"in Java we'd need threads and locks\", \"Python would require\", \"Node.js uses callbacks\"\n- Problem-solution language: \"the issue was\", \"we needed to\", \"goroutines allowed us to\", \"channels let us\"\n- Constraints mentioned: \"memory efficiency\", \"latency\", \"CPU cores\", \"resource limits\"\n- Understanding of trade-offs: \"simpler than\", \"more efficient than\", \"easier to reason about\""
    },
    {
      "question_id": "Q3_PRODUCTION_OPERATIONS",
      "question_text": "Tell me about a time when a Go service you built or maintained caused an operational problem in production—maybe a performance issue, memory leak, unexpected behavior, or something else that surprised you. How did you debug it and what did you learn?",
      "category": "production and operations",
      "difficulty": "hard",
      "time_allocation_minutes": 4,
      "follow_up_questions": [],
      "what_to_look_for": "STRONG ANSWER: Candidate describes a specific production incident they personally debugged. They explain the symptoms, their debugging approach (profiling tools, logs, metrics, etc.), the root cause, and the fix. They demonstrate understanding of Go-specific operational concerns (memory management, goroutine leaks, CPU profiling, pprof, etc.). They reflect on what they learned and how they prevented recurrence. Examples: goroutine leaks, memory leaks, connection pool exhaustion, race conditions, panic in production.\n\nWEAK ANSWER: No specific incident or extremely vague (\"we had a bug\"). Cannot explain debugging approach. Blames external factors without investigating. No mention of tools or methodology used to diagnose the problem. No reflection on root cause or prevention. Answers that show they didn't personally debug it.\n\nMINIMUM VIABLE EVIDENCE: Candidate must describe a specific production problem, explain at least one debugging technique or tool they used (logs, profiling, metrics, etc.), identify a root cause, and explain what they changed to fix it. Must demonstrate personal involvement in debugging.\n\nTRANSCRIPT SIGNALS TO LISTEN FOR:\n- Go-specific tools mentioned: \"pprof\", \"heap profile\", \"goroutine dump\", \"trace\", \"delve debugger\"\n- Debugging methodology: \"checked logs\", \"looked at metrics\", \"profiled CPU/memory\", \"added instrumentation\", \"tested hypothesis\"\n- Root cause identification: \"goroutine leak\", \"unbounded channel\", \"connection leak\", \"panic loop\", \"memory not freed\", \"race condition\"\n- Operational context: \"high traffic time\", \"under load\", \"after deployment\", \"intermittent\", \"production alert\"\n- Problem-solving language: \"we noticed\", \"we investigated\", \"we found\", \"the issue was\", \"we fixed it by\"\n- Learning and prevention: \"we added monitoring\", \"we refactored\", \"we added tests\", \"we changed our deployment process\", \"we learned to\"\n- Specific technical details: \"goroutines kept growing\", \"memory usage climbed\", \"latency increased\", \"CPU spiked\", \"requests started failing\"\n- Constraints and trade-offs: \"had to roll back\", \"had to debug in production\", \"couldn't reproduce locally\", \"had limited time\""
    }
  ]
}